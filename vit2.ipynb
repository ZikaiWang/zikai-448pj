{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "import timm\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from typing import Optional"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "'''\n",
    "https://storage.googleapis.com/vit_models/augreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_224.npz\n",
    "vit_base_patch16_224\n",
    "'''\n",
    "\n",
    "net = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "torch.save(net,\"checkpoint/vit_base_patch16_224.pth\")\n",
    "#net.head = torch.nn.Linear(net.head.in_features, 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "<_io.TextIOWrapper name='checkpoint/vit_base_patch16_224.pth' mode='r' encoding='cp936'>"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"checkpoint/vit_base_patch16_224.pth\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (norm_pre): Identity()\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (fc_norm): Identity()\n",
      "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"checkpoint/vit_base_patch16_224.pth\")\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DTYPE = torch.float32"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "transform1 = torchvision.transforms.Compose([\n",
    "                                                torchvision.transforms.RandomCrop(32, padding=4),\n",
    "                                                torchvision.transforms.Resize(32),\n",
    "                                                torchvision.transforms.RandomHorizontalFlip(),\n",
    "                                                torchvision.transforms.ToTensor(),\n",
    "                                                #torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "                                                torchvision.transforms.ConvertImageDtype(DTYPE)\n",
    "                                             ])\n",
    "\n",
    "transform2 = torchvision.transforms.Compose([\n",
    "                                                torchvision.transforms.Resize(32),\n",
    "                                                torchvision.transforms.ToTensor(),\n",
    "                                                #torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "                                                torchvision.transforms.ConvertImageDtype(DTYPE)\n",
    "                                             ])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 75\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "#lr = 0.002\n",
    "lr = 0.01\n",
    "momentum = 0.9"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform1)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=8, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "from torchvision.models.vision_transformer import Encoder\n",
    "from functools import partial\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer as per https://arxiv.org/abs/2010.11929.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int,\n",
    "        patch_size: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float = 0.0,\n",
    "        attention_dropout: float = 0.0,\n",
    "        num_classes: int = 1000,\n",
    "        representation_size: Optional[int] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if image_size % patch_size != 0:\n",
    "            print(\"Input shape indivisible by patch size!\")\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "        self.representation_size = representation_size\n",
    "\n",
    "\n",
    "        self.conv_proj = nn.Conv2d(\n",
    "            in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "        seq_length = (image_size // patch_size) ** 2\n",
    "\n",
    "        # Add a class token\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "        seq_length += 1\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            seq_length,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            hidden_dim,\n",
    "            mlp_dim,\n",
    "            dropout,\n",
    "            attention_dropout,\n",
    "        )\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.heads = nn.Sequential(nn.Linear(hidden_dim, num_classes))\n",
    "\n",
    "        if isinstance(self.conv_proj, nn.Conv2d):\n",
    "            # Init the patchify stem\n",
    "            fan_in = self.conv_proj.in_channels * self.conv_proj.kernel_size[0] * self.conv_proj.kernel_size[1]\n",
    "            nn.init.trunc_normal_(self.conv_proj.weight, std=math.sqrt(1 / fan_in))\n",
    "            if self.conv_proj.bias is not None:\n",
    "                nn.init.zeros_(self.conv_proj.bias)\n",
    "        elif self.conv_proj.conv_last is not None and isinstance(self.conv_proj.conv_last, nn.Conv2d):\n",
    "            # Init the last 1x1 conv of the conv stem\n",
    "            nn.init.normal_(\n",
    "                self.conv_proj.conv_last.weight, mean=0.0, std=math.sqrt(2.0 / self.conv_proj.conv_last.out_channels)\n",
    "            )\n",
    "            if self.conv_proj.conv_last.bias is not None:\n",
    "                nn.init.zeros_(self.conv_proj.conv_last.bias)\n",
    "\n",
    "        if hasattr(self.heads, \"pre_logits\") and isinstance(self.heads.pre_logits, nn.Linear):\n",
    "            fan_in = self.heads.pre_logits.in_features\n",
    "            nn.init.trunc_normal_(self.heads.pre_logits.weight, std=math.sqrt(1 / fan_in))\n",
    "            nn.init.zeros_(self.heads.pre_logits.bias)\n",
    "\n",
    "\n",
    "    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        n, c, h, w = x.shape\n",
    "        p = self.patch_size\n",
    "        torch._assert(h == self.image_size, f\"Wrong image height! Expected {self.image_size} but got {h}!\")\n",
    "        torch._assert(w == self.image_size, f\"Wrong image width! Expected {self.image_size} but got {w}!\")\n",
    "        n_h = h // p\n",
    "        n_w = w // p\n",
    "\n",
    "        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n",
    "        x = self.conv_proj(x)\n",
    "        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n",
    "        x = x.reshape(n, self.hidden_dim, n_h * n_w)\n",
    "\n",
    "        # (n, hidden_dim, (n_h * n_w)) -> (n, (n_h * n_w), hidden_dim)\n",
    "        # The self attention layer expects inputs in the format (N, S, E)\n",
    "        # where S is the source sequence length, N is the batch size, E is the\n",
    "        # embedding dimension\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Reshape and permute the input tensor\n",
    "        x = self._process_input(x)\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Expand the class token to the full batch\n",
    "        batch_class_token = self.class_token.expand(n, -1, -1)\n",
    "        x = torch.cat([batch_class_token, x], dim=1)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Classifier \"token\" as used by standard language architectures\n",
    "        x = x[:, 0]\n",
    "\n",
    "        x = self.heads(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "    model = VisionTransformer(\n",
    "        image_size=image_size,\n",
    "        patch_size=patch_size,\n",
    "        num_layers=num_layers,\n",
    "        num_heads=num_heads,\n",
    "        hidden_dim=hidden_dim,\n",
    "        mlp_dim=mlp_dim,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    return _vision_transformer(\n",
    "        patch_size=16,\n",
    "        num_layers=12,\n",
    "        num_heads=12,\n",
    "        hidden_dim=768,\n",
    "        mlp_dim=3072,\n",
    "        weights=weights,\n",
    "        progress=progress,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "'''\n",
    "model = VisionTransformer(\n",
    "    image_size=32,\n",
    "    patch_size=8,\n",
    "    num_layers=12,\n",
    "    num_heads=12,\n",
    "    hidden_dim=768,\n",
    "    mlp_dim=3072,\n",
    "    dropout=0.2,\n",
    "    attention_dropout=0.2,\n",
    "    num_classes=10\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x25286084648>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(torchvision.transforms.ToPILImage()(trainset[0][0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x25286364e48>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(  trainset[0][0].permute(1,2,0)  )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (conv_proj): Conv2d(3, 768, kernel_size=(8, 8), stride=(8, 8))\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      (encoder_layer_0): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.2, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_1): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.2, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_2): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.2, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_3): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.2, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_4): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.2, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_5): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.2, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_6): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.2, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_7): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.2, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_8): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.2, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_9): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.2, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_10): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.2, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_11): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.2, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (heads): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#net = torchvision.models.vit_b_16(weights=\"IMAGENET1K_SWAG_LINEAR_V1\")\n",
    "#net.heads.head = torch.nn.Linear(net.heads.head.in_features, 10)\n",
    "\n",
    "net = model\n",
    "print(net)\n",
    "net.heads[0] = torch.nn.Linear(net.heads[0].in_features, 10)\n",
    "\n",
    "net = net.train().to(device=DEVICE, dtype=DTYPE)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "def tran(epoch):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print('[%d] loss: %.3f' % (epoch + 1, running_loss))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def test():\n",
    "    net.eval()\n",
    "    all_counter=0\n",
    "    correct_counter=0\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        out = net(inputs)\n",
    "        out = out.detach().cpu().argmax(1)\n",
    "        t = labels.cpu()\n",
    "        for m in range(len(t)):\n",
    "            all_counter += 1\n",
    "            if t[m] == out[m]:\n",
    "                correct_counter += 1\n",
    "\n",
    "    print(correct_counter, all_counter, correct_counter / all_counter)\n",
    "    return (correct_counter / all_counter)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 1423.992\n",
      "3006 10000 0.3006\n",
      "best:  0.3006  in NO:  0\n",
      "[2] loss: 1180.378\n",
      "3919 10000 0.3919\n",
      "best:  0.3919  in NO:  1\n",
      "[3] loss: 1102.811\n",
      "4300 10000 0.43\n",
      "best:  0.43  in NO:  2\n",
      "[4] loss: 1051.154\n",
      "4638 10000 0.4638\n",
      "best:  0.4638  in NO:  3\n",
      "[5] loss: 1015.340\n",
      "4666 10000 0.4666\n",
      "best:  0.4666  in NO:  4\n",
      "[6] loss: 989.000\n",
      "4936 10000 0.4936\n",
      "best:  0.4936  in NO:  5\n",
      "[7] loss: 965.087\n",
      "5085 10000 0.5085\n",
      "best:  0.5085  in NO:  6\n",
      "[8] loss: 952.096\n",
      "5032 10000 0.5032\n",
      "[9] loss: 931.464\n",
      "5203 10000 0.5203\n",
      "best:  0.5203  in NO:  8\n",
      "[10] loss: 920.734\n",
      "5312 10000 0.5312\n",
      "best:  0.5312  in NO:  9\n",
      "[11] loss: 908.146\n",
      "5334 10000 0.5334\n",
      "best:  0.5334  in NO:  10\n",
      "[12] loss: 900.586\n",
      "5306 10000 0.5306\n",
      "[13] loss: 885.477\n",
      "5416 10000 0.5416\n",
      "best:  0.5416  in NO:  12\n",
      "[14] loss: 878.281\n",
      "5446 10000 0.5446\n",
      "best:  0.5446  in NO:  13\n",
      "[15] loss: 866.882\n",
      "5543 10000 0.5543\n",
      "best:  0.5543  in NO:  14\n",
      "[16] loss: 857.515\n",
      "5503 10000 0.5503\n",
      "[17] loss: 847.622\n",
      "5520 10000 0.552\n",
      "[18] loss: 843.237\n",
      "5641 10000 0.5641\n",
      "best:  0.5641  in NO:  17\n",
      "[19] loss: 837.750\n",
      "5629 10000 0.5629\n",
      "[20] loss: 830.829\n",
      "5554 10000 0.5554\n"
     ]
    }
   ],
   "source": [
    "correctRate = 0\n",
    "for i in range(epochs):\n",
    "    tran(i)\n",
    "    r = test()\n",
    "    if(r > correctRate):\n",
    "        correctRate = r\n",
    "        print(\"best: \", r , \" in NO: \", i)\n",
    "        torch.save(net.cpu(),\"checkpoint/vit.pth\")\n",
    "        net = net.to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n\\n[1] loss: 1039.766\\n5726 10000 0.5726\\nbest:  0.5726  in NO:  0\\n[2] loss: 494.575\\n8055 10000 0.8055\\nbest:  0.8055  in NO:  1\\n[3] loss: 246.069\\n8669 10000 0.8669\\nbest:  0.8669  in NO:  2\\n[4] loss: 170.940\\n9136 10000 0.9136\\nbest:  0.9136  in NO:  3\\n[5] loss: 122.860\\n9395 10000 0.9395\\nbest:  0.9395  in NO:  4\\n[6] loss: 101.786\\n9409 10000 0.9409\\nbest:  0.9409  in NO:  5\\n[7] loss: 67.831\\n9451 10000 0.9451\\nbest:  0.9451  in NO:  6\\n[8] loss: 55.644\\n9499 10000 0.9499\\nbest:  0.9499  in NO:  7\\n[9] loss: 42.075\\n9469 10000 0.9469\\n[10] loss: 38.531\\n9587 10000 0.9587\\nbest:  0.9587  in NO:  9\\n[11] loss: 25.020\\n9565 10000 0.9565\\n[12] loss: 20.583\\n9613 10000 0.9613\\nbest:  0.9613  in NO:  11\\n[13] loss: 15.646\\n9642 10000 0.9642\\nbest:  0.9642  in NO:  12\\n[14] loss: 13.876\\n9616 10000 0.9616\\n[15] loss: 10.449\\n9660 10000 0.966\\nbest:  0.966  in NO:  14\\n[16] loss: 8.551\\n9692 10000 0.9692\\nbest:  0.9692  in NO:  15\\n[17] loss: 4.337\\n9681 10000 0.9681\\n[18] loss: 4.242\\n9740 10000 0.974\\nbest:  0.974  in NO:  17\\n[19] loss: 1.846\\n9707 10000 0.9707\\n[20] loss: 3.379\\n9695 10000 0.9695\\n\\n\\n'"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "[1] loss: 1039.766\n",
    "5726 10000 0.5726\n",
    "best:  0.5726  in NO:  0\n",
    "[2] loss: 494.575\n",
    "8055 10000 0.8055\n",
    "best:  0.8055  in NO:  1\n",
    "[3] loss: 246.069\n",
    "8669 10000 0.8669\n",
    "best:  0.8669  in NO:  2\n",
    "[4] loss: 170.940\n",
    "9136 10000 0.9136\n",
    "best:  0.9136  in NO:  3\n",
    "[5] loss: 122.860\n",
    "9395 10000 0.9395\n",
    "best:  0.9395  in NO:  4\n",
    "[6] loss: 101.786\n",
    "9409 10000 0.9409\n",
    "best:  0.9409  in NO:  5\n",
    "[7] loss: 67.831\n",
    "9451 10000 0.9451\n",
    "best:  0.9451  in NO:  6\n",
    "[8] loss: 55.644\n",
    "9499 10000 0.9499\n",
    "best:  0.9499  in NO:  7\n",
    "[9] loss: 42.075\n",
    "9469 10000 0.9469\n",
    "[10] loss: 38.531\n",
    "9587 10000 0.9587\n",
    "best:  0.9587  in NO:  9\n",
    "[11] loss: 25.020\n",
    "9565 10000 0.9565\n",
    "[12] loss: 20.583\n",
    "9613 10000 0.9613\n",
    "best:  0.9613  in NO:  11\n",
    "[13] loss: 15.646\n",
    "9642 10000 0.9642\n",
    "best:  0.9642  in NO:  12\n",
    "[14] loss: 13.876\n",
    "9616 10000 0.9616\n",
    "[15] loss: 10.449\n",
    "9660 10000 0.966\n",
    "best:  0.966  in NO:  14\n",
    "[16] loss: 8.551\n",
    "9692 10000 0.9692\n",
    "best:  0.9692  in NO:  15\n",
    "[17] loss: 4.337\n",
    "9681 10000 0.9681\n",
    "[18] loss: 4.242\n",
    "9740 10000 0.974\n",
    "best:  0.974  in NO:  17\n",
    "[19] loss: 1.846\n",
    "9707 10000 0.9707\n",
    "[20] loss: 3.379\n",
    "9695 10000 0.9695\n",
    "\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
